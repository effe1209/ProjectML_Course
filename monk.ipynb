{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225a147a",
   "metadata": {},
   "source": [
    "# Monk Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from model.network import NeuralNetwork\n",
    "from model.trainer import Trainer\n",
    "from model.losses import Loss\n",
    "from utils import DataLoader\n",
    "from utils import load_monk, plot_curves\n",
    "from utils.model_selection_helpers import count_parameters\n",
    "from model.activations import sigmoid\n",
    "from model.losses import mse\n",
    "from utils.grid_search import grid_search_monk\n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(8) #reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d599dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the configurations to test for the training\n",
    "# Neural Network architectures\n",
    "INPUT_NEURONS = 17\n",
    "OUTPUT_NEURONS = 1\n",
    "\n",
    "HIDDEN_LAYER_SIZES = [4, 8]\n",
    "HIDDEN_LAYERS_COUNTS = [1, 2]\n",
    "INTERNAL_ACTIVATIONS = ['tanh', 'leaky relu', 'relu']\n",
    "OUTPUT_ACTIVATIONS_AND_LOSS = [('sigmoid', 'mse'), ('identity', 'binary cross entropy sigmoid')]\n",
    "NEURAL_NETWORK_CONFIGURATIONS = []\n",
    "\n",
    "for hidden_layers_count in HIDDEN_LAYERS_COUNTS:\n",
    "    for hidden_layer_size in HIDDEN_LAYER_SIZES:\n",
    "        for internal_activation in INTERNAL_ACTIVATIONS:\n",
    "            for output_activation, loss_function in OUTPUT_ACTIVATIONS_AND_LOSS:\n",
    "                architecture = [INPUT_NEURONS] + [hidden_layer_size] * hidden_layers_count + [OUTPUT_NEURONS]\n",
    "                activations = [internal_activation] * hidden_layers_count + [output_activation]\n",
    "                NEURAL_NETWORK_CONFIGURATIONS.append((architecture, activations, loss_function))\n",
    "\n",
    "# Training parameters\n",
    "ETA_CONFIGURATIONS = [0.25, 0.1]\n",
    "LAMBDA_CONFIGURATIONS = [0, 1e-1, 1e-2, 1e-3]\n",
    "ALPHA_CONFIGURATIONS = [0, 0.5, 0.9]\n",
    "BATCH_SIZES = [32, -1]\n",
    "\n",
    "# Cross-validation parameters\n",
    "K_FOLDS= 5\n",
    "EPOCHS = 500\n",
    "EARLY_STOPPING_PATIENCE = 50\n",
    "\n",
    "# All possible configurations are tuples (NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE)\n",
    "CONFIGURATIONS = []\n",
    "\n",
    "for NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F in NEURAL_NETWORK_CONFIGURATIONS:\n",
    "  for ETA in ETA_CONFIGURATIONS:\n",
    "    for LAMBDA in LAMBDA_CONFIGURATIONS:\n",
    "      for ALPHA in ALPHA_CONFIGURATIONS:\n",
    "        for BATCH_SIZE in BATCH_SIZES:\n",
    "            config = (NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE)\n",
    "            CONFIGURATIONS.append(config)\n",
    "LEN_CONFIGURATIONS = len(CONFIGURATIONS)\n",
    "\n",
    "print(f\"Total configurations: {LEN_CONFIGURATIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37d2f1",
   "metadata": {},
   "source": [
    "## Monk 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98155d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the dataset and create k folds\n",
    "PATH_TRAIN = 'data/monk/monks-1.train'\n",
    "PATH_TEST = 'data/monk/monks-1.test'\n",
    "X_train_full, y_train_full, X_test, y_test = load_monk(PATH_TRAIN, PATH_TEST)\n",
    "monk_dataset_1 = DataLoader(X_train_full, y_train_full)\n",
    "k_fold = monk_dataset_1.k_fold(k = K_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8bf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chosen configuration\n",
    "'''Config index: ([17, 4, 1], ['relu', 'identity'], 'binary cross entropy sigmoid', 0.1, 0.001, 0.9, 32), Mean Accuracy: 100.0%,\n",
    "            training instability coeff validation: 7.251121972242489, \n",
    "            training instability coeff train: 18.125499760799777, \n",
    "            training loss-val loss diff: 8.502494598628806, \n",
    "            Mean Epochs: 182'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [17, 4, 1], ['relu', 'identity'], 'binary cross entropy sigmoid', 0.1, 0.001, 0.9, 32\n",
    "EPOCHS = 182\n",
    "\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_full,\n",
    "    y_train=y_train_full,\n",
    "    X_val = X_test,\n",
    "    y_val=y_test,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False, plot_monk=True, plot_title='Monk 1 ')\n",
    "\n",
    "#plot curves\n",
    "plot_curves(train_loss_vector, test_loss_vector, 'loss (BCE)', 'test', title = 'Monk 1 Loss', save_plots=True)\n",
    "print(f\"Train loss BCE: {train_loss_vector[-1]}, Test loss BCE: {test_loss_vector[-1],}\")\n",
    "\n",
    "out = nn.forward(X_train_full)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "if LOSS_F == 'binary cross entropy sigmoid':\n",
    "  out = sigmoid(out)\n",
    "\n",
    "print(f\"Train MSE: {np.mean(mse(out, y_train_full))}\")\n",
    "predictions = np.round(out)\n",
    "print(f\"Train Accuracy: {np.mean(predictions == y_train_full) * 100}%\")\n",
    "\n",
    "# val accuracy, we can't early stop because we are using the test set as final evaluation, we can't pick the best nn, we have to use the last nn\n",
    "out = nn.forward(X_test)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "if LOSS_F == 'binary cross entropy sigmoid':\n",
    "  out = sigmoid(out)\n",
    "print(f\"Test MSE: {np.mean(mse(out, y_test))}\")\n",
    "\n",
    "predictions = np.round(out)\n",
    "print(f\"Validation Accuracy: {np.mean(predictions == y_test) * 100}%\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696dbdf",
   "metadata": {},
   "source": [
    "## Monk 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the dataset and create k folds\n",
    "PATH_TRAIN = 'data/monk/monks-2.train'\n",
    "PATH_TEST = 'data/monk/monks-2.test'\n",
    "X_train_full, y_train_full, X_test, y_test = load_monk(PATH_TRAIN, PATH_TEST)\n",
    "monk_dataset_2 = DataLoader(X_train_full, y_train_full)\n",
    "k_fold = monk_dataset_2.k_fold(k = K_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chosen configuration\n",
    "'''Config index: ([17, 4, 1], ['leaky relu', 'identity'], 'binary cross entropy sigmoid', 0.25, 0, 0.9, -1), Mean Accuracy: 100.0%,\n",
    "            training instability coeff validation: 0.21730799802185136, \n",
    "            training instability coeff train: 0.05647236271957169, \n",
    "            training loss-val loss diff: 5.611568195799458, \n",
    "            Mean Epochs: 500'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [17, 4, 1], ['leaky relu', 'identity'], 'binary cross entropy sigmoid', 0.25, 0, 0.9, -1\n",
    "EPOCHS = 500\n",
    "\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_full,\n",
    "    y_train=y_train_full,\n",
    "    X_val = X_test,\n",
    "    y_val=y_test,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False, plot_monk=True, plot_title='Monk 2 ')\n",
    "\n",
    "#plot curves\n",
    "plot_curves(train_loss_vector, test_loss_vector, 'loss (BCE)', 'test', title = 'Monk 2 Loss', save_plots=True)\n",
    "\n",
    "print(f\"Train loss: {train_loss_vector[-1]}, Test loss: {test_loss_vector[-1],}\")\n",
    "\n",
    "out = nn.forward(X_train_full)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "if LOSS_F == 'binary cross entropy sigmoid':\n",
    "  out = sigmoid(out)\n",
    "\n",
    "predictions = np.round(out)\n",
    "print(f\"Train Accuracy: {np.mean(predictions == y_train_full) * 100}%\")\n",
    "# val accuracy, we can't early stop because we are using the test set as final evaluation, we can't pick the best nn, we have to use the last nn\n",
    "out = nn.forward(X_test)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "if LOSS_F == 'binary cross entropy sigmoid':\n",
    "  out = sigmoid(out)\n",
    "\n",
    "predictions = np.round(out)\n",
    "print(f\"Validation Accuracy: {np.mean(predictions == y_test) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791311d7",
   "metadata": {},
   "source": [
    "## Monk 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the dataset and create k folds\n",
    "PATH_TRAIN = 'data/monk/monks-3.train'\n",
    "PATH_TEST = 'data/monk/monks-3.test'\n",
    "X_train_full, y_train_full, X_test, y_test = load_monk(PATH_TRAIN, PATH_TEST)\n",
    "monk_dataset_3 = DataLoader(X_train_full, y_train_full)\n",
    "k_fold = monk_dataset_3.k_fold(k = K_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chosen configuration\n",
    "'''Config index: ([17, 4, 1], ['tanh', 'sigmoid'], 'mse', 0.25, 0.01, 0.5, -1), Mean Accuracy: 93.5%,\n",
    "            training instability coeff validation: 0.0007768047741556949, \n",
    "            training instability coeff train: 0.0, \n",
    "            training loss-val loss diff: 1.9177302710431792, \n",
    "            Mean Epochs: 133\n",
    "'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [17, 4, 1], ['tanh', 'sigmoid'], 'mse', 0.25, 0.01, 0.5, -1\n",
    "EPOCHS = 133\n",
    "\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_full,\n",
    "    y_train=y_train_full,\n",
    "    X_val = X_test,\n",
    "    y_val=y_test,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False, plot_monk=True, plot_title='Monk 3')\n",
    "\n",
    "#plot curves\n",
    "plot_curves(train_loss_vector, test_loss_vector, 'loss (BCE)', 'test', title = 'Monk 3 Loss', save_plots=True)\n",
    "\n",
    "print(f\"Train loss: {train_loss_vector[-1]}, Test loss: {test_loss_vector[-1],}\")\n",
    "\n",
    "out = nn.forward(X_train_full)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "if LOSS_F == 'binary cross entropy sigmoid':\n",
    "  out = sigmoid(out)\n",
    "\n",
    "predictions = np.round(out)\n",
    "print(f\"Train Accuracy: {np.mean(predictions == y_train_full) * 100}%\")\n",
    "# val accuracy, we can't early stop because we are using the test set as final evaluation, we can't pick the best nn, we have to use the last nn\n",
    "out = nn.forward(X_test)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "if LOSS_F == 'binary cross entropy sigmoid':\n",
    "  out = sigmoid(out)\n",
    "\n",
    "predictions = np.round(out)\n",
    "print(f\"Validation Accuracy: {np.mean(predictions == y_test) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c53c9",
   "metadata": {},
   "source": [
    "## Monk 3 no reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chosen overfitting configuration\n",
    "'''Config index: ([17, 8, 8, 1], ['relu', 'relu', 'identity'], 'binary cross entropy sigmoid', 0.25, 0, 0.5, -1), Mean Accuracy: 94.3%,\n",
    "        training instability coeff validation: 0.5101067216534323, \n",
    "        training instability coeff train: 0.0035634011571540245, \n",
    "        training loss-val loss diff: 14.215801300638915, \n",
    "        Mean Epochs: 178\n",
    "'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set, we further increase the architecture complexity to force overfitting\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [17, 8, 8, 1], ['relu', 'relu', 'identity'], 'binary cross entropy sigmoid', 0.25, 0, 0.5, -1\n",
    "EPOCHS = 178\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_full,\n",
    "    y_train=y_train_full,\n",
    "    X_val = X_test,\n",
    "    y_val=y_test,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False, plot_monk=True, plot_title='Monk 3 no regularization')\n",
    "\n",
    "#plot curves\n",
    "plot_curves(train_loss_vector, test_loss_vector, 'loss (BCE)', 'test', title = 'Monk 3 no regularization Loss', save_plots=True)\n",
    "\n",
    "# train accuracy\n",
    "out = nn.forward(X_train_full)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "if LOSS_F == 'binary cross entropy sigmoid':\n",
    "  out = sigmoid(out)\n",
    "\n",
    "predictions = np.round(out)\n",
    "print(f\"Train Accuracy: {np.mean(predictions == y_train_full) * 100}%\")\n",
    "\n",
    "# val accuracy, we can't early stop because we are using the test set as final evaluation, we can't pick the best nn, we have to use the last nn\n",
    "out = nn.forward(X_test)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "if LOSS_F == 'binary cross entropy sigmoid':\n",
    "  out = sigmoid(out)\n",
    "\n",
    "predictions = np.round(out)\n",
    "print(f\"Test Accuracy: {np.mean(predictions == y_test) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d2e99",
   "metadata": {},
   "source": [
    "## Mean for different initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50843122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the dataset and create k folds\n",
    "PATH_TRAIN = 'data/monk/monks-1.train'\n",
    "PATH_TEST = 'data/monk/monks-1.test'\n",
    "X_train_full, y_train_full, X_test, y_test = load_monk(PATH_TRAIN, PATH_TEST)\n",
    "\n",
    "### chosen configuration\n",
    "'''Config index: ([17, 4, 1], ['relu', 'identity'], 'binary cross entropy sigmoid', 0.1, 0.001, 0.9, 32), Mean Accuracy: 100.0%,\n",
    "            training instability coeff validation: 7.251121972242489, \n",
    "            training instability coeff train: 18.125499760799777, \n",
    "            training loss-val loss diff: 8.502494598628806, \n",
    "            Mean Epochs: 182'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [17, 4, 1], ['relu', 'identity'], 'binary cross entropy sigmoid', 0.1, 0.001, 0.9, 32\n",
    "EPOCHS = 182\n",
    "\n",
    "N_INITIALIZATIONS = 10\n",
    "\n",
    "Train_loss_BCE= 0\n",
    "Test_loss_BCE= 0\n",
    "Train_MSE= 0\n",
    "Train_Accuracy= 0\n",
    "Test_MSE= 0\n",
    "Validation_Accuracy= 0\n",
    "\n",
    "for _ in range(N_INITIALIZATIONS):\n",
    "    nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "    trainer = Trainer(\n",
    "        nn=nn,\n",
    "        loss=Loss(LOSS_F),\n",
    "        X_train=X_train_full,\n",
    "        y_train=y_train_full,\n",
    "        X_val = X_test,\n",
    "        y_val=y_test,\n",
    "        epochs=EPOCHS,\n",
    "        early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "        eta=ETA,                   # Learning rate iniziale\n",
    "        lam=LAMBDA,                # L2\n",
    "        alpha=ALPHA,               # Momentum\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_batches=True\n",
    "    )\n",
    "    # still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "    best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False)\n",
    "\n",
    "    #plot curves\n",
    "    Train_loss_BCE += train_loss_vector[-1]\n",
    "    Test_loss_BCE += test_loss_vector[-1]\n",
    "\n",
    "    out = nn.forward(X_train_full)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "    if LOSS_F == 'binary cross entropy sigmoid':\n",
    "        out = sigmoid(out)\n",
    "\n",
    "    Train_MSE += np.mean(mse(out, y_train_full))\n",
    "    predictions = np.round(out)\n",
    "    Train_Accuracy += np.mean(predictions == y_train_full) * 100\n",
    "    # val accuracy, we can't early stop because we are using the test set as final evaluation, we can't pick the best nn, we have to use the last nn\n",
    "    out = nn.forward(X_test)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "    if LOSS_F == 'binary cross entropy sigmoid':\n",
    "        out = sigmoid(out)\n",
    "    Test_MSE += np.mean(mse(out, y_test))\n",
    "\n",
    "    predictions = np.round(out)\n",
    "    Validation_Accuracy += np.mean(predictions == y_test) * 100\n",
    "\n",
    "print(f\"Mean results after {N_INITIALIZATIONS} initialization\")\n",
    "print(f\"Train MSE: {Train_MSE/N_INITIALIZATIONS}\")\n",
    "print(f\"Train Accuracy: {Train_Accuracy/N_INITIALIZATIONS}%\")\n",
    "print(f\"Train loss BCE: {Train_loss_BCE/N_INITIALIZATIONS}, Test loss BCE: {Test_loss_BCE/N_INITIALIZATIONS}\")\n",
    "print(f\"Test MSE: {Test_MSE/N_INITIALIZATIONS}\")\n",
    "print(f\"Test Accuracy: {Validation_Accuracy/N_INITIALIZATIONS}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463dd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the dataset and create k folds\n",
    "PATH_TRAIN = 'data/monk/monks-2.train'\n",
    "PATH_TEST = 'data/monk/monks-2.test'\n",
    "X_train_full, y_train_full, X_test, y_test = load_monk(PATH_TRAIN, PATH_TEST)\n",
    "\n",
    "### chosen configuration\n",
    "'''Config index: ([17, 4, 1], ['leaky relu', 'identity'], 'binary cross entropy sigmoid', 0.25, 0, 0.9, -1), Mean Accuracy: 100.0%,\n",
    "            training instability coeff validation: 0.21730799802185136, \n",
    "            training instability coeff train: 0.05647236271957169, \n",
    "            training loss-val loss diff: 5.611568195799458, \n",
    "            Mean Epochs: 500'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [17, 4, 1], ['leaky relu', 'identity'], 'binary cross entropy sigmoid', 0.25, 0, 0.9, -1\n",
    "EPOCHS = 500\n",
    "\n",
    "N_INITIALIZATIONS = 10\n",
    "\n",
    "Train_loss_BCE= 0\n",
    "Test_loss_BCE= 0\n",
    "Train_MSE= 0\n",
    "Train_Accuracy= 0\n",
    "Test_MSE= 0\n",
    "Validation_Accuracy= 0\n",
    "\n",
    "for _ in range(N_INITIALIZATIONS):\n",
    "    nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "    trainer = Trainer(\n",
    "        nn=nn,\n",
    "        loss=Loss(LOSS_F),\n",
    "        X_train=X_train_full,\n",
    "        y_train=y_train_full,\n",
    "        X_val = X_test,\n",
    "        y_val=y_test,\n",
    "        epochs=EPOCHS,\n",
    "        early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "        eta=ETA,                   # Learning rate iniziale\n",
    "        lam=LAMBDA,                # L2\n",
    "        alpha=ALPHA,               # Momentum\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_batches=True\n",
    "    )\n",
    "    # still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "    best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False)\n",
    "\n",
    "    #plot curves\n",
    "    Train_loss_BCE += train_loss_vector[-1]\n",
    "    Test_loss_BCE += test_loss_vector[-1]\n",
    "\n",
    "    out = nn.forward(X_train_full)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "    if LOSS_F == 'binary cross entropy sigmoid':\n",
    "        out = sigmoid(out)\n",
    "\n",
    "    Train_MSE += np.mean(mse(out, y_train_full))\n",
    "    predictions = np.round(out)\n",
    "    Train_Accuracy += np.mean(predictions == y_train_full) * 100\n",
    "    # val accuracy, we can't early stop because we are using the test set as final evaluation, we can't pick the best nn, we have to use the last nn\n",
    "    out = nn.forward(X_test)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "    if LOSS_F == 'binary cross entropy sigmoid':\n",
    "        out = sigmoid(out)\n",
    "    Test_MSE += np.mean(mse(out, y_test))\n",
    "\n",
    "    predictions = np.round(out)\n",
    "    Validation_Accuracy += np.mean(predictions == y_test) * 100\n",
    "\n",
    "print(f\"Mean results after {N_INITIALIZATIONS} initialization\")\n",
    "print(f\"Train MSE: {Train_MSE/N_INITIALIZATIONS}\")\n",
    "print(f\"Train Accuracy: {Train_Accuracy/N_INITIALIZATIONS}%\")\n",
    "print(f\"Train loss BCE: {Train_loss_BCE/N_INITIALIZATIONS}, Test loss BCE: {Test_loss_BCE/N_INITIALIZATIONS}\")\n",
    "print(f\"Test MSE: {Test_MSE/N_INITIALIZATIONS}\")\n",
    "print(f\"Test Accuracy: {Validation_Accuracy/N_INITIALIZATIONS}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470970d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the dataset and create k folds\n",
    "PATH_TRAIN = 'data/monk/monks-3.train'\n",
    "PATH_TEST = 'data/monk/monks-3.test'\n",
    "X_train_full, y_train_full, X_test, y_test = load_monk(PATH_TRAIN, PATH_TEST)\n",
    "monk_dataset_3 = DataLoader(X_train_full, y_train_full)\n",
    "k_fold = monk_dataset_3.k_fold(k = K_FOLDS)\n",
    "\n",
    "### chosen configuration\n",
    "'''Config index: ([17, 4, 1], ['tanh', 'sigmoid'], 'mse', 0.25, 0.01, 0.5, -1), Mean Accuracy: 93.5%,\n",
    "            training instability coeff validation: 0.0007768047741556949, \n",
    "            training instability coeff train: 0.0, \n",
    "            training loss-val loss diff: 1.9177302710431792, \n",
    "            Mean Epochs: 133\n",
    "'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [17, 4, 1], ['tanh', 'sigmoid'], 'mse', 0.25, 0.01, 0.5, -1\n",
    "EPOCHS = 133\n",
    "\n",
    "N_INITIALIZATIONS = 10\n",
    "\n",
    "Train_loss_BCE= 0\n",
    "Test_loss_BCE= 0\n",
    "Train_MSE= 0\n",
    "Train_Accuracy= 0\n",
    "Test_MSE= 0\n",
    "Validation_Accuracy= 0\n",
    "\n",
    "for _ in range(N_INITIALIZATIONS):\n",
    "    nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "    trainer = Trainer(\n",
    "        nn=nn,\n",
    "        loss=Loss(LOSS_F),\n",
    "        X_train=X_train_full,\n",
    "        y_train=y_train_full,\n",
    "        X_val = X_test,\n",
    "        y_val=y_test,\n",
    "        epochs=EPOCHS,\n",
    "        early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "        eta=ETA,                   # Learning rate iniziale\n",
    "        lam=LAMBDA,                # L2\n",
    "        alpha=ALPHA,               # Momentum\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_batches=True\n",
    "    )\n",
    "    # still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "    best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False)\n",
    "\n",
    "    #plot curves\n",
    "    Train_loss_BCE += train_loss_vector[-1]\n",
    "    Test_loss_BCE += test_loss_vector[-1]\n",
    "\n",
    "    out = nn.forward(X_train_full)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "    if LOSS_F == 'binary cross entropy sigmoid':\n",
    "        out = sigmoid(out)\n",
    "\n",
    "    Train_MSE += np.mean(mse(out, y_train_full))\n",
    "    predictions = np.round(out)\n",
    "    Train_Accuracy += np.mean(predictions == y_train_full) * 100\n",
    "    # val accuracy, we can't early stop because we are using the test set as final evaluation, we can't pick the best nn, we have to use the last nn\n",
    "    out = nn.forward(X_test)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "    if LOSS_F == 'binary cross entropy sigmoid':\n",
    "        out = sigmoid(out)\n",
    "    Test_MSE += np.mean(mse(out, y_test))\n",
    "\n",
    "    predictions = np.round(out)\n",
    "    Validation_Accuracy += np.mean(predictions == y_test) * 100\n",
    "\n",
    "print(f\"Mean results after {N_INITIALIZATIONS} initialization\")\n",
    "print(f\"Train MSE: {Train_MSE/N_INITIALIZATIONS}\")\n",
    "print(f\"Train Accuracy: {Train_Accuracy/N_INITIALIZATIONS}%\")\n",
    "print(f\"Train loss BCE: {Train_loss_BCE/N_INITIALIZATIONS}, Test loss BCE: {Test_loss_BCE/N_INITIALIZATIONS}\")\n",
    "print(f\"Test MSE: {Test_MSE/N_INITIALIZATIONS}\")\n",
    "print(f\"Test Accuracy: {Validation_Accuracy/N_INITIALIZATIONS}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1623acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chosen overfitting configuration\n",
    "'''Config index: ([17, 8, 8, 1], ['relu', 'relu', 'identity'], 'binary cross entropy sigmoid', 0.25, 0, 0.5, -1), Mean Accuracy: 94.3%,\n",
    "        training instability coeff validation: 0.5101067216534323, \n",
    "        training instability coeff train: 0.0035634011571540245, \n",
    "        training loss-val loss diff: 14.215801300638915, \n",
    "        Mean Epochs: 178\n",
    "'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set, we further increase the architecture complexity to force overfitting\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [17, 8, 8, 1], ['relu', 'relu', 'identity'], 'binary cross entropy sigmoid', 0.25, 0, 0.5, -1\n",
    "EPOCHS = 178\n",
    "\n",
    "N_INITIALIZATIONS = 10\n",
    "\n",
    "Train_loss_BCE= 0\n",
    "Test_loss_BCE= 0\n",
    "Train_MSE= 0\n",
    "Train_Accuracy= 0\n",
    "Test_MSE= 0\n",
    "Validation_Accuracy= 0\n",
    "\n",
    "for _ in range(N_INITIALIZATIONS):\n",
    "    nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "    trainer = Trainer(\n",
    "        nn=nn,\n",
    "        loss=Loss(LOSS_F),\n",
    "        X_train=X_train_full,\n",
    "        y_train=y_train_full,\n",
    "        X_val = X_test,\n",
    "        y_val=y_test,\n",
    "        epochs=EPOCHS,\n",
    "        early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "        eta=ETA,                   # Learning rate iniziale\n",
    "        lam=LAMBDA,                # L2\n",
    "        alpha=ALPHA,               # Momentum\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_batches=True\n",
    "    )\n",
    "    # still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "    best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False)\n",
    "\n",
    "    #plot curves\n",
    "    Train_loss_BCE += train_loss_vector[-1]\n",
    "    Test_loss_BCE += test_loss_vector[-1]\n",
    "\n",
    "    out = nn.forward(X_train_full)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "    if LOSS_F == 'binary cross entropy sigmoid':\n",
    "        out = sigmoid(out)\n",
    "\n",
    "    Train_MSE += np.mean(mse(out, y_train_full))\n",
    "    predictions = np.round(out)\n",
    "    Train_Accuracy += np.mean(predictions == y_train_full) * 100\n",
    "    # val accuracy, we can't early stop because we are using the test set as final evaluation, we can't pick the best nn, we have to use the last nn\n",
    "    out = nn.forward(X_test)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "    if LOSS_F == 'binary cross entropy sigmoid':\n",
    "        out = sigmoid(out)\n",
    "    Test_MSE += np.mean(mse(out, y_test))\n",
    "\n",
    "    predictions = np.round(out)\n",
    "    Validation_Accuracy += np.mean(predictions == y_test) * 100\n",
    "\n",
    "print(f\"Mean results after {N_INITIALIZATIONS} initialization\")\n",
    "print(f\"Train MSE: {Train_MSE/N_INITIALIZATIONS}\")\n",
    "print(f\"Train Accuracy: {Train_Accuracy/N_INITIALIZATIONS}%\")\n",
    "print(f\"Train loss BCE: {Train_loss_BCE/N_INITIALIZATIONS}, Test loss BCE: {Test_loss_BCE/N_INITIALIZATIONS}\")\n",
    "print(f\"Test MSE: {Test_MSE/N_INITIALIZATIONS}\")\n",
    "print(f\"Test Accuracy: {Validation_Accuracy/N_INITIALIZATIONS}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
