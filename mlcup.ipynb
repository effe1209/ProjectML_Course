{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc4a735",
   "metadata": {},
   "source": [
    "# ML Cup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from model.network import NeuralNetwork\n",
    "from model.trainer import Trainer\n",
    "from model.losses import Loss, mee\n",
    "from utils import DataLoader\n",
    "from utils import plot_curves\n",
    "from utils import StandardScaler\n",
    "from utils.model_selection_helpers import instability_coeff, tran_val_diff, count_parameters\n",
    "from utils.grid_search import grid_search_mlcup\n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "PATH = 'data/ML CUP/ML-CUP25-TR.csv'\n",
    "df = pd.read_csv(PATH, comment='#', header=None)\n",
    "\n",
    "dataset = np.array(df)\n",
    "X = dataset[:, 1:-4]\n",
    "y = dataset[:, -4:]\n",
    "\n",
    "print(f\"X.shape: {X.shape}, y.shape: {y.shape} \")\n",
    "train_test_dataset = DataLoader(X, y)\n",
    "X_train, y_train, X_test, y_test = train_test_dataset.train_val_split(portion = 0.8, shuffle = True)\n",
    "print(f\"X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}, X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}\")\n",
    "train_val_dataset = DataLoader(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30add27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the configurations to test for the training\n",
    "# Neural Network architectures\n",
    "INPUT_NEURONS = 12\n",
    "OUTPUT_NEURONS = 4\n",
    "\n",
    "HIDDEN_LAYER_SIZES = [8, 16, 32]\n",
    "HIDDEN_LAYERS_COUNTS = [1, 2]\n",
    "INTERNAL_ACTIVATIONS = ['tanh', 'leaky relu', 'relu']\n",
    "OUTPUT_ACTIVATIONS_AND_LOSS = [('identity', 'mse')]\n",
    "NEURAL_NETWORK_CONFIGURATIONS = []\n",
    "\n",
    "for hidden_layers_count in HIDDEN_LAYERS_COUNTS:\n",
    "    for hidden_layer_size in HIDDEN_LAYER_SIZES:\n",
    "        for internal_activation in INTERNAL_ACTIVATIONS:\n",
    "            for output_activation, loss_function in OUTPUT_ACTIVATIONS_AND_LOSS:\n",
    "                architecture = [INPUT_NEURONS] + [hidden_layer_size] * hidden_layers_count + [OUTPUT_NEURONS]\n",
    "                activations = [internal_activation] * hidden_layers_count + [output_activation]\n",
    "                NEURAL_NETWORK_CONFIGURATIONS.append((architecture, activations, loss_function))\n",
    "\n",
    "# Training parameters\n",
    "ETA_CONFIGURATIONS = [0.1, 0.05]\n",
    "LAMBDA_CONFIGURATIONS = [0, 1e-1, 1e-3] # we have to make them small because they are independent of eta\n",
    "ALPHA_CONFIGURATIONS = [0, 0.5, 0.9]\n",
    "BATCH_SIZES = [64, -1] \n",
    "\n",
    "# Cross-validation parameters\n",
    "K_FOLDS= 5\n",
    "EPOCHS = 1000\n",
    "EARLY_STOPPING_PATIENCE = 100\n",
    "\n",
    "# All possible configurations are tuples (NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE)\n",
    "CONFIGURATIONS = []\n",
    "\n",
    "for NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F in NEURAL_NETWORK_CONFIGURATIONS:\n",
    "  for ETA in ETA_CONFIGURATIONS:\n",
    "    for LAMBDA in LAMBDA_CONFIGURATIONS:\n",
    "      for ALPHA in ALPHA_CONFIGURATIONS:\n",
    "        for BATCH_SIZE in BATCH_SIZES:\n",
    "            config = (NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE)\n",
    "            CONFIGURATIONS.append(config)\n",
    "LEN_CONFIGURATIONS = len(CONFIGURATIONS)\n",
    "\n",
    "print(f\"Total configurations: {LEN_CONFIGURATIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_25(CONFIG_DICTIONARY, CONFIG_DICTIONARY_EPOCHS, CONFIG_DICTIONARY_INSTABILITY_TRAIN, CONFIG_DICTIONARY_INSTABILITY_VAL, CONFIG_DICTIONARY_TRAIN_LOSS_DIFF, CONFIG_DICTIONARY_TEST_LOSS, CONFIG_DICTIONARY_TRAIN_LOSS, min_mee = 100.0, sec_metric = 'val stab'):\n",
    "    # print the top 25 configurations, sorting criterias: 1 avg mee\n",
    "    #sometimes gradient explodes and mee is nan\n",
    "    assert sec_metric in ['val stab', 'overfit', 'parameters'], \"choose a parameter between 'val stab', 'overfit', 'parameters'\"\n",
    "        \n",
    "    valid_id = []\n",
    "    for i in CONFIG_DICTIONARY:\n",
    "        if not np.isnan(CONFIG_DICTIONARY[i]):\n",
    "            valid_id.append(i)\n",
    "        \n",
    "    if sec_metric == 'val stab':\n",
    "        TOP_25_CONFIGS_INDEXES = sorted(valid_id, key=lambda i: (max(min_mee, CONFIG_DICTIONARY[i] / K_FOLDS), CONFIG_DICTIONARY_INSTABILITY_VAL[i]))[:25]\n",
    "    if sec_metric == 'overfit':\n",
    "        TOP_25_CONFIGS_INDEXES = sorted(valid_id, key=lambda i: (max(min_mee, CONFIG_DICTIONARY[i] / K_FOLDS), CONFIG_DICTIONARY_TRAIN_LOSS_DIFF[i]))[:25]\n",
    "    if sec_metric == 'parameters':\n",
    "        TOP_25_CONFIGS_INDEXES = sorted(valid_id, key=lambda i: (max(min_mee, CONFIG_DICTIONARY[i] / K_FOLDS), count_parameters(CONFIGURATIONS[i][0])))[:25]\n",
    "    \n",
    "    print(\"Top 25 configurations:\")\n",
    "    for i in TOP_25_CONFIGS_INDEXES:\n",
    "        MEE = CONFIG_DICTIONARY[i]/ K_FOLDS\n",
    "        print(f'''Config index: {CONFIGURATIONS[i]}, Avg Epochs: {CONFIG_DICTIONARY_EPOCHS[i] // K_FOLDS}, Mean MEE (best of each epoch): {MEE},\n",
    "            training instability coeff validation: {CONFIG_DICTIONARY_INSTABILITY_VAL[i] / K_FOLDS}, \n",
    "            training instability coeff train: {CONFIG_DICTIONARY_INSTABILITY_TRAIN[i] / K_FOLDS}, \n",
    "            training loss-val loss diff: {CONFIG_DICTIONARY_TRAIN_LOSS_DIFF[i] / K_FOLDS}, \n",
    "            Mean Epochs: {CONFIG_DICTIONARY_EPOCHS[i] // K_FOLDS},\n",
    "            Mean Test Loss (last epoch nn according to val): {np.mean(CONFIG_DICTIONARY_TEST_LOSS[i])},\n",
    "            Std Test Loss (last epoch nn according to val): {np.std(CONFIG_DICTIONARY_TEST_LOSS[i])},\n",
    "            Mean Train Loss (last wepoch nn according to val): {np.mean(CONFIG_DICTIONARY_TRAIN_LOSS[i])},\n",
    "            Std Train Loss (last epoch nn according to val): {np.std(CONFIG_DICTIONARY_TRAIN_LOSS[i])}\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2237b9",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b504de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = train_val_dataset.k_fold(k = K_FOLDS)\n",
    "grid_s_outputs =  grid_search_mlcup(LEN_CONFIGURATIONS, CONFIGURATIONS, k_fold, EPOCHS, EARLY_STOPPING_PATIENCE)\n",
    "CONFIG_DICTIONARY, CONFIG_DICTIONARY_EPOCHS, CONFIG_DICTIONARY_INSTABILITY_TRAIN, CONFIG_DICTIONARY_INSTABILITY_VAL, CONFIG_DICTIONARY_TRAIN_LOSS_DIFF, CONFIG_DICTIONARY_TEST_LOSS, CONFIG_DICTIONARY_TRAIN_LOSS = grid_s_outputs\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae3851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_25(CONFIG_DICTIONARY, CONFIG_DICTIONARY_EPOCHS, CONFIG_DICTIONARY_INSTABILITY_TRAIN, CONFIG_DICTIONARY_INSTABILITY_VAL, CONFIG_DICTIONARY_TRAIN_LOSS_DIFF, CONFIG_DICTIONARY_TEST_LOSS, CONFIG_DICTIONARY_TRAIN_LOSS, min_mee= 21.5, sec_metric = 'val stab') ## if many NN have MEE , 23 -> we choose the most stableML_Cup_internal_test.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e75bd",
   "metadata": {},
   "source": [
    "## Some Experiments with validation set\n",
    "We test some configurations and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the train set in train and val to see how some configurations behave\n",
    "X_train_exp, y_train_exp, X_test_exp, y_test_exp = train_val_dataset.train_val_split(portion = 0.8, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaad1ab",
   "metadata": {},
   "source": [
    "### Lowest mean test loss at last epoch\n",
    "Test Loss (last epoch) is too high and unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Config index: ([12, 32, 32, 4], ['leaky relu', 'leaky relu', 'identity'], 'mse', 0.05, 0.001, 0.9, 64), Avg Epochs: 479, Mean MEE (best of each epoch): 19.806613807812433,\n",
    "            training instability coeff validation: 18.86797998785415, \n",
    "            training instability coeff train: 7.955502350675206, \n",
    "            training loss-val loss diff: 66.0931309768494, \n",
    "            Mean Epochs: 479,\n",
    "            Mean Test Loss (last epoch nn according to val): 20.622951242722888,\n",
    "            Std Test Loss (last epoch nn according to val): 1.3390524715087282,-\n",
    "            Mean Train Loss (last wepoch nn according to val): 15.695105853244893,\n",
    "            Std Train Loss (last epoch nn according to val): 1.0725833487389098\n",
    "'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [12, 32, 32, 4], ['leaky relu', 'leaky relu', 'identity'], 'mse', 0.05, 0.001, 0.9, 64\n",
    "EPOCHS = 479\n",
    "\n",
    "#scaling\n",
    "X_scaler = StandardScaler(X_train_exp)\n",
    "X_train_scaled = X_scaler.transform(X_train_exp)\n",
    "X_val_scaled = X_scaler.transform(X_test_exp)\n",
    "y_scaler = StandardScaler(y_train_exp)\n",
    "y_train_scaled = y_scaler.transform(y_train_exp)\n",
    "y_val_scaled = y_scaler.transform(y_test_exp)\n",
    "\n",
    "#training\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train_scaled,\n",
    "    X_val = X_val_scaled,\n",
    "    y_val=y_val_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False, plot_mee= True, plot_title = 'MEE of NN with best Mean MEE on grid search', y_rescaler = y_scaler)\n",
    "\n",
    "#plot curves\n",
    "plot_curves(np.array(train_loss_vector), np.array(test_loss_vector), 'loss (MSE)', 'test', title = 'Loss NN with best Mean MEE on grid search', save_plots=True)\n",
    "\n",
    "# Train MAE\n",
    "out = nn.forward(X_train_scaled)[-1][-1] \n",
    "out = y_scaler.inverse_transform(out)\n",
    "train_loss = mee(y_train_exp, out)\n",
    "print(f\"Train MEE: {np.mean(train_loss)}\")\n",
    "\n",
    "# Test MAE\n",
    "out = nn.forward(X_val_scaled)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "out = y_scaler.inverse_transform(out)\n",
    "\n",
    "test_loss = mee(y_test_exp, out)\n",
    "print(f\"Test MEE: {np.mean(test_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737cb8e",
   "metadata": {},
   "source": [
    "### Good Mean test loss at last epoch, good std test loss, low train instability coefficient\n",
    "unluckly sometimes the error spikes, making the configuration not suitable for the final choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Config index: ([12, 32, 32, 4], ['tanh', 'tanh', 'identity'], 'mse', 0.05, 0, 0.9, -1), Avg Epochs: 717, Mean MEE (best of each epoch): 21.04609489087083,\n",
    "            training instability coeff validation: 2.2650823081050215, \n",
    "            training instability coeff train: 1.9364435724808948, \n",
    "            training loss-val loss diff: 76.91487938495433, \n",
    "            Mean Epochs: 717,\n",
    "            Mean Test Loss (last epoch nn according to val): 21.19851130557733,\n",
    "            Std Test Loss (last epoch nn according to val): 0.48895085831205704,\n",
    "            Mean Train Loss (last wepoch nn according to val): 17.5854051739146,\n",
    "            Std Train Loss (last epoch nn according to val): 1.3248901449466473'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [12, 32, 32, 4], ['tanh', 'tanh', 'identity'], 'mse', 0.05, 0, 0.9, -1\n",
    "EPOCHS = 717\n",
    "\n",
    "#scaling\n",
    "X_scaler = StandardScaler(X_train_exp)\n",
    "X_train_scaled = X_scaler.transform(X_train_exp)\n",
    "X_val_scaled = X_scaler.transform(X_test_exp)\n",
    "y_scaler = StandardScaler(y_train_exp)\n",
    "y_train_scaled = y_scaler.transform(y_train_exp)\n",
    "y_val_scaled = y_scaler.transform(y_test_exp)\n",
    "\n",
    "#training\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train_scaled,\n",
    "    X_val = X_val_scaled,\n",
    "    y_val=y_val_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False, plot_mee= True, plot_title = 'Unstable Neural Network MEE', y_rescaler = y_scaler)\n",
    "\n",
    "#plot curves\n",
    "plot_curves(np.array(train_loss_vector), np.array(test_loss_vector), 'loss (MSE)', 'test', title = 'Unstable Neural Network Loss', save_plots=True)\n",
    "\n",
    "# Train MAE\n",
    "out = nn.forward(X_train_scaled)[-1][-1] \n",
    "out = y_scaler.inverse_transform(out)\n",
    "train_loss = mee(y_train_exp, out)\n",
    "print(f\"Train MEE: {np.mean(train_loss)}\")\n",
    "\n",
    "# Test MAE\n",
    "out = nn.forward(X_val_scaled)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "out = y_scaler.inverse_transform(out)\n",
    "\n",
    "test_loss = mee(y_test_exp, out)\n",
    "print(f\"Test MEE: {np.mean(test_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c336a",
   "metadata": {},
   "source": [
    "### Stable, small, neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14363c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Config index: ([12, 16, 4], ['tanh', 'identity'], 'mse', 0.1, 0.001, 0.9, -1), Avg Epochs: 827, Mean MEE (best of each epoch): 21.42464664114587,\n",
    "            training instability coeff validation: 0.673773199356931, \n",
    "            training instability coeff train: 0.5399767396542754, \n",
    "            training loss-val loss diff: 70.78310831831926, \n",
    "            Mean Epochs: 827,\n",
    "            Mean Test Loss (last epoch nn according to val): 21.41966790695764,\n",
    "            Std Test Loss (last epoch nn according to val): 0.7682862643016937,\n",
    "            Mean Train Loss (last wepoch nn according to val): 19.258783842464872,\n",
    "            Std Train Loss (last epoch nn according to val): 0.2986657311213303\"\"\"\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [12, 16, 4], ['tanh', 'identity'], 'mse', 0.1, 0.001, 0.9, -1\n",
    "EPOCHS = 827\n",
    "\n",
    "#scaling\n",
    "X_scaler = StandardScaler(X_train_exp)\n",
    "X_train_scaled = X_scaler.transform(X_train_exp)\n",
    "X_val_scaled = X_scaler.transform(X_test_exp)\n",
    "y_scaler = StandardScaler(y_train_exp)\n",
    "y_train_scaled = y_scaler.transform(y_train_exp)\n",
    "y_val_scaled = y_scaler.transform(y_test_exp)\n",
    "\n",
    "#training\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train_scaled,\n",
    "    X_val = X_val_scaled,\n",
    "    y_val=y_val_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False, plot_mee= True, plot_title = 'Stable Small Neural Network MEE', y_rescaler = y_scaler)\n",
    "\n",
    "#plot curves\n",
    "plot_curves(np.array(train_loss_vector), np.array(test_loss_vector), 'loss (MSE)', 'test', title = 'Stable Small Neural Network Loss', save_plots=True)\n",
    "\n",
    "# Train MAE\n",
    "out = nn.forward(X_train_scaled)[-1][-1] \n",
    "out = y_scaler.inverse_transform(out)\n",
    "train_loss = mee(y_train_exp, out)\n",
    "print(f\"Train MEE: {np.mean(train_loss)}\")\n",
    "\n",
    "# Test MAE\n",
    "out = nn.forward(X_val_scaled)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "out = y_scaler.inverse_transform(out)\n",
    "\n",
    "test_loss = mee(y_test_exp, out)\n",
    "print(f\"Test MEE: {np.mean(test_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f2473",
   "metadata": {},
   "source": [
    "## Internal Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d44d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chosen configuration -> chosen for: mean mee, stabil valid, low std test loss\n",
    "\"\"\"Config index: ([12, 16, 4], ['tanh', 'identity'], 'mse', 0.1, 0.001, 0.9, -1), Avg Epochs: 827, Mean MEE (best of each epoch): 21.42464664114587,\n",
    "            training instability coeff validation: 0.673773199356931, \n",
    "            training instability coeff train: 0.5399767396542754, \n",
    "            training loss-val loss diff: 70.78310831831926, \n",
    "            Mean Epochs: 827,\n",
    "            Mean Test Loss (last epoch nn according to val): 21.41966790695764,\n",
    "            Std Test Loss (last epoch nn according to val): 0.7682862643016937,\n",
    "            Mean Train Loss (last wepoch nn according to val): 19.258783842464872,\n",
    "            Std Train Loss (last epoch nn according to val): 0.2986657311213303\"\"\"\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [12, 16, 4], ['tanh', 'identity'], 'mse', 0.1, 0.001, 0.9, -1\n",
    "EPOCHS = 827\n",
    "\n",
    "#scaling\n",
    "X_scaler = StandardScaler(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_val_scaled = X_scaler.transform(X_test)\n",
    "y_scaler = StandardScaler(y_train)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_val_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "#training\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train_scaled,\n",
    "    X_val = X_val_scaled,\n",
    "    y_val=y_val_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False, plot_mee= True, plot_title = 'Neural Network MEE', y_rescaler = y_scaler)\n",
    "\n",
    "#plot curves\n",
    "plot_curves(np.array(train_loss_vector), np.array(test_loss_vector), 'loss (MSE)', 'test', title = 'Neural Network Loss', save_plots=True)\n",
    "\n",
    "# Train MAE\n",
    "out = nn.forward(X_train_scaled)[-1][-1] \n",
    "out = y_scaler.inverse_transform(out)\n",
    "train_loss = mee(y_train, out)\n",
    "print(f\"Train MEE: {np.mean(train_loss)}\")\n",
    "\n",
    "# Test MAE\n",
    "out = nn.forward(X_val_scaled)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "out = y_scaler.inverse_transform(out)\n",
    "\n",
    "test_loss = mee(y_test, out)\n",
    "print(f\"Test MEE: {np.mean(test_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea093a81",
   "metadata": {},
   "source": [
    "## Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574992d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "PATH_TR = 'data/ML CUP/ML-CUP25-TR.csv'\n",
    "PATH_TS = 'data/ML CUP/ML-CUP25-TS.csv'\n",
    "\n",
    "df = pd.read_csv(PATH_TR, comment='#', header=None)\n",
    "\n",
    "dataset = np.array(df)\n",
    "X_tr = dataset[:, 1:-4]\n",
    "y = dataset[:, -4:]\n",
    "\n",
    "df = pd.read_csv(PATH_TS, comment='#', header=None)\n",
    "\n",
    "dataset = np.array(df)\n",
    "X_ts = dataset[:, 1:]\n",
    "\n",
    "print(f\"X_tr.shape: {X_tr.shape}, y.shape: {y.shape},  X_ts.shape: {X_ts.shape}\")\n",
    "dataset = DataLoader(X_tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chosen configuration -> chosen for: mean mee, stabil valid, low std test loss\n",
    "\"\"\"Config index: ([12, 16, 4], ['tanh', 'identity'], 'mse', 0.1, 0.001, 0.9, -1), Avg Epochs: 827, Mean MEE (best of each epoch): 21.42464664114587,\n",
    "            training instability coeff validation: 0.673773199356931, \n",
    "            training instability coeff train: 0.5399767396542754, \n",
    "            training loss-val loss diff: 70.78310831831926, \n",
    "            Mean Epochs: 827,\n",
    "            Mean Test Loss (last epoch nn according to val): 21.41966790695764,\n",
    "            Std Test Loss (last epoch nn according to val): 0.7682862643016937,\n",
    "            Mean Train Loss (last wepoch nn according to val): 19.258783842464872,\n",
    "            Std Train Loss (last epoch nn according to val): 0.2986657311213303\"\"\"\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE = [12, 16, 4], ['tanh', 'identity'], 'mse', 0.1, 0.001, 0.9, -1\n",
    "EPOCHS = 827\n",
    "\n",
    "#scaling\n",
    "X_scaler = StandardScaler(X_tr)\n",
    "X_train_scaled = X_scaler.transform(X_tr)\n",
    "X_val_scaled = X_scaler.transform(X_ts)\n",
    "y_scaler = StandardScaler(y)\n",
    "y_train_scaled = y_scaler.transform(y)\n",
    "\n",
    "#training\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=True)\n",
    "\n",
    "# Train MAE\n",
    "out = nn.forward(X_val_scaled)[-1][-1] \n",
    "out = y_scaler.inverse_transform(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d78f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nome team \"Fraleli\"\n",
    "# membri \"Alessandro Tesi, Elia Bocini, Francesco Fiaschi\"\n",
    "# data = \"21/01/2026\"\n",
    "\n",
    "df_submission = pd.DataFrame(out)\n",
    "df_submission.index = df_submission.index + 1  #adjust index to follow cup 25 csv\n",
    "df_submission.to_csv('FRALELI_ML-CUP25-TS.csv') ### RICORDATI DI AGGIUNGERE I COMMENTI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
