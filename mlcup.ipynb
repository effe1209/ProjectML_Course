{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc4a735",
   "metadata": {},
   "source": [
    "# ML Cup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from model.network import NeuralNetwork\n",
    "from model.trainer import Trainer\n",
    "from model.losses import Loss, mee\n",
    "from utils import DataLoader\n",
    "from utils import plot_curves\n",
    "from utils import StandardScaler\n",
    "from utils.model_selection_helpers import instability_coeff, tran_val_diff, count_parameters\n",
    "from utils.grid_search import grid_search_mlcup\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "PATH = 'data/ML CUP/ML-CUP25-TR.csv'\n",
    "df = pd.read_csv(PATH, comment='#', header=None)\n",
    "\n",
    "dataset = np.array(df)\n",
    "X = dataset[:, 1:-4]\n",
    "y = dataset[:, -4:]\n",
    "\n",
    "print(f\"X.shape: {X.shape}, y.shape: {y.shape} \")\n",
    "train_test_dataset = DataLoader(X, y)\n",
    "X_train, y_train, X_test, y_test = train_test_dataset.train_val_split(portion = 0.8, shuffle = True)\n",
    "print(f\"X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}, X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}\")\n",
    "train_val_dataset = DataLoader(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30add27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the configurations to test for the training\n",
    "# Neural Network architectures\n",
    "INPUT_NEURONS = 12\n",
    "OUTPUT_NEURONS = 4\n",
    "\n",
    "HIDDEN_LAYER_SIZES = [8, 16, 32]\n",
    "HIDDEN_LAYERS_COUNTS = [1, 2]\n",
    "INTERNAL_ACTIVATIONS = ['tanh', 'leaky relu', 'relu']\n",
    "OUTPUT_ACTIVATIONS_AND_LOSS = [('identity', 'mse')]\n",
    "NEURAL_NETWORK_CONFIGURATIONS = []\n",
    "\n",
    "for hidden_layers_count in HIDDEN_LAYERS_COUNTS:\n",
    "    for hidden_layer_size in HIDDEN_LAYER_SIZES:\n",
    "        for internal_activation in INTERNAL_ACTIVATIONS:\n",
    "            for output_activation, loss_function in OUTPUT_ACTIVATIONS_AND_LOSS:\n",
    "                architecture = [INPUT_NEURONS] + [hidden_layer_size] * hidden_layers_count + [OUTPUT_NEURONS]\n",
    "                activations = [internal_activation] * hidden_layers_count + [output_activation]\n",
    "                NEURAL_NETWORK_CONFIGURATIONS.append((architecture, activations, loss_function))\n",
    "\n",
    "# Training parameters\n",
    "ETA_CONFIGURATIONS =[0.001, 0.0001]\n",
    "LAMBDA_CONFIGURATIONS =[1e-4, 5e-4, 1e-5] # we have to make them small because they are independent of eta\n",
    "ALPHA_CONFIGURATIONS = [0.5, 0.9]\n",
    "BATCH_SIZES =  [64, 128] \n",
    "\n",
    "# Cross-validation parameters\n",
    "K_FOLDS= 5\n",
    "EPOCHS = 250\n",
    "EARLY_STOPPING_PATIENCE = 50\n",
    "\n",
    "# All possible configurations are tuples (NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE)\n",
    "CONFIGURATIONS = []\n",
    "\n",
    "for NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F in NEURAL_NETWORK_CONFIGURATIONS:\n",
    "  for ETA in ETA_CONFIGURATIONS:\n",
    "    for LAMBDA in LAMBDA_CONFIGURATIONS:\n",
    "      for ALPHA in ALPHA_CONFIGURATIONS:\n",
    "        for BATCH_SIZE in BATCH_SIZES:\n",
    "            config = (NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE)\n",
    "            CONFIGURATIONS.append(config)\n",
    "LEN_CONFIGURATIONS = len(CONFIGURATIONS)\n",
    "\n",
    "print(f\"Total configurations: {LEN_CONFIGURATIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_25(CONFIG_DICTIONARY, CONFIG_DICTIONARY_EPOCHS, CONFIG_DICTIONARY_INSTABILITY_TRAIN, CONFIG_DICTIONARY_INSTABILITY_VAL, CONFIG_DICTIONARY_TRAIN_LOSS_DIFF, CONFIG_DICTIONARY_TEST_LOSS, min_mee = 100.0):\n",
    "    # print the top 25 configurations, sorting criterias: 1 avg mee\n",
    "    #sometimes gradient explodes and mee is nan\n",
    "    valid_id = []\n",
    "    for i in CONFIG_DICTIONARY:\n",
    "        if not np.isnan(CONFIG_DICTIONARY[i]):\n",
    "            valid_id.append(i)\n",
    "            \n",
    "    TOP_25_CONFIGS_INDEXES = sorted(valid_id, key=lambda i: (max(min_mee, CONFIG_DICTIONARY[i] / K_FOLDS), CONFIG_DICTIONARY_TRAIN_LOSS_DIFF[i],count_parameters(CONFIGURATIONS[i][0]), CONFIG_DICTIONARY_INSTABILITY_VAL[i]))[:25]\n",
    "    print(\"Top 25 configurations:\")\n",
    "    for i in TOP_25_CONFIGS_INDEXES:\n",
    "        MEE = CONFIG_DICTIONARY[i]/ K_FOLDS\n",
    "        print(f'''Config index: {CONFIGURATIONS[i]}, Avg Epochs: {CONFIG_DICTIONARY_EPOCHS[i] // K_FOLDS}, Mean MEE: {MEE}%,\n",
    "            training instability coeff validation: {CONFIG_DICTIONARY_INSTABILITY_VAL[i] / K_FOLDS}, \n",
    "            training instability coeff train: {CONFIG_DICTIONARY_INSTABILITY_TRAIN[i] / K_FOLDS}, \n",
    "            training loss-val loss diff: {CONFIG_DICTIONARY_TRAIN_LOSS_DIFF[i] / K_FOLDS}, \n",
    "            Mean Epochs: {CONFIG_DICTIONARY_EPOCHS[i] // K_FOLDS},\n",
    "            Mean Test Loss (not rescaled): {np.mean(CONFIG_DICTIONARY_TEST_LOSS[i])},\n",
    "            Std Test Loss (not rescaled): {np.std(CONFIG_DICTIONARY_TEST_LOSS[i])}\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2237b9",
   "metadata": {},
   "source": [
    "## Neural Network for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b504de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = train_val_dataset.k_fold(k = K_FOLDS)\n",
    "grid_s_outputs =  grid_search_mlcup(LEN_CONFIGURATIONS, CONFIGURATIONS, k_fold, EPOCHS, EARLY_STOPPING_PATIENCE)\n",
    "CONFIG_DICTIONARY, CONFIG_DICTIONARY_EPOCHS, CONFIG_DICTIONARY_INSTABILITY_TRAIN, CONFIG_DICTIONARY_INSTABILITY_VAL, CONFIG_DICTIONARY_TRAIN_LOSS_DIFF, CONFIG_DICTIONARY_TEST_LOSS = grid_s_outputs\n",
    "clear_output(wait=False)\n",
    "print_top_25(CONFIG_DICTIONARY, CONFIG_DICTIONARY_EPOCHS, CONFIG_DICTIONARY_INSTABILITY_TRAIN, CONFIG_DICTIONARY_INSTABILITY_VAL, CONFIG_DICTIONARY_TRAIN_LOSS_DIFF, CONFIG_DICTIONARY_TEST_LOSS, min_mee= 23.0) ## if many NN have MEE , 23 -> we choose the most stableML_Cup_internal_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da923571",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_25(CONFIG_DICTIONARY, CONFIG_DICTIONARY_EPOCHS, CONFIG_DICTIONARY_INSTABILITY_TRAIN, CONFIG_DICTIONARY_INSTABILITY_VAL, CONFIG_DICTIONARY_TRAIN_LOSS_DIFF, CONFIG_DICTIONARY_TEST_LOSS, min_mee= 22.0) ## if many NN have MEE , 23 -> we choose the most stableML_Cup_internal_test.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f2473",
   "metadata": {},
   "source": [
    "## Internal Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d44d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chosen configuration\n",
    "'''Config index: ([12, 32, 4], ['relu', 'identity'], 'mse', 0.001, 0.0001, 0.9, 64), Avg Epochs: 164, Mean MEE: 21.960007052590793%,\n",
    "            training instability coeff validation: 6.345182397467081, \n",
    "            training instability coeff train: 1.6182371515598581, \n",
    "            training loss-val loss diff: 15.633324404887933, \n",
    "            Mean Epochs: 164,\n",
    "            Mean Test Loss (not rescaled): 0.47513341866662573,\n",
    "            Std Test Loss (not rescaled): 0.03454318930158234\n",
    "'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE =[12, 32, 4], ['relu', 'identity'], 'mse', 0.001, 0.0001, 0.9, 64\n",
    "EPOCHS = 164 - EARLY_STOPPING_PATIENCE\n",
    "\n",
    "#scaling\n",
    "X_scaler = StandardScaler(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_val_scaled = X_scaler.transform(X_test)\n",
    "y_scaler = StandardScaler(y_train)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_val_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "#training\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train_scaled,\n",
    "    X_val = X_val_scaled,\n",
    "    y_val=y_val_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=False, plot_mee= True, plot_title = 'Neural Network MEE (not rescaled)')\n",
    "\n",
    "#plot curves\n",
    "plot_curves(np.array(train_loss_vector), np.array(test_loss_vector), 'loss (MSE)', 'test', title = 'Neural Network Loss', save_plots=True)\n",
    "\n",
    "# Train MAE\n",
    "out = nn.forward(X_train_scaled)[-1][-1] \n",
    "out = y_scaler.inverse_transform(out)\n",
    "train_loss = mee(y_train, out)\n",
    "print(f\"Train MEE: {np.mean(train_loss)}\")\n",
    "\n",
    "# Test MAE\n",
    "out = nn.forward(X_val_scaled)[-1][-1]  #have to use the last trained nn, otherwise is data leakage\n",
    "out = y_scaler.inverse_transform(out)\n",
    "\n",
    "test_loss = mee(y_test, out)\n",
    "print(f\"Test MEE: {np.mean(test_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea093a81",
   "metadata": {},
   "source": [
    "## Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574992d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "PATH_TR = 'data/ML CUP/ML-CUP25-TR.csv'\n",
    "PATH_TS = 'data/ML CUP/ML-CUP25-TS.csv'\n",
    "\n",
    "df = pd.read_csv(PATH_TR, comment='#', header=None)\n",
    "\n",
    "dataset = np.array(df)\n",
    "X_tr = dataset[:, 1:-4]\n",
    "y = dataset[:, -4:]\n",
    "\n",
    "df = pd.read_csv(PATH_TS, comment='#', header=None)\n",
    "\n",
    "dataset = np.array(df)\n",
    "X_ts = dataset[:, 1:]\n",
    "\n",
    "print(f\"X_tr.shape: {X_tr.shape}, y.shape: {y.shape},  X_ts.shape: {X_ts.shape}\")\n",
    "dataset = DataLoader(X_tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chosen configuration\n",
    "'''Config index: ([12, 32, 4], ['relu', 'identity'], 'mse', 0.001, 0.0001, 0.9, 64), Avg Epochs: 164, Mean MEE: 21.960007052590793%,\n",
    "            training instability coeff validation: 6.345182397467081, \n",
    "            training instability coeff train: 1.6182371515598581, \n",
    "            training loss-val loss diff: 15.633324404887933, \n",
    "            Mean Epochs: 164,\n",
    "            Mean Test Loss (not rescaled): 0.47513341866662573,\n",
    "            Std Test Loss (not rescaled): 0.03454318930158234\n",
    "'''\n",
    "\n",
    "# Choose the best configuration and train it on the full training set, evaluate on the test set\n",
    "NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA, BATCH_SIZE =[12, 32, 4], ['relu', 'identity'], 'mse', 0.001, 0.0001, 0.9, 64\n",
    "EPOCHS = 164 - EARLY_STOPPING_PATIENCE\n",
    "\n",
    "#scaling\n",
    "X_scaler = StandardScaler(X_tr)\n",
    "X_train_scaled = X_scaler.transform(X_tr)\n",
    "X_val_scaled = X_scaler.transform(X_ts)\n",
    "y_scaler = StandardScaler(y)\n",
    "y_train_scaled = y_scaler.transform(y)\n",
    "\n",
    "#training\n",
    "nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION)\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    loss=Loss(LOSS_F),\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping=EPOCHS + 1, #can't early stop because we are using the test set as final evaluation, the +1 is to be extra sure\n",
    "    eta=ETA,                   # Learning rate iniziale\n",
    "    lam=LAMBDA,                # L2\n",
    "    alpha=ALPHA,               # Momentum\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_batches=True\n",
    ")\n",
    "# still returns the best nn, but we don-t use it for accuracy calculation nor early stopping otherwise it is data leakage\n",
    "best_nn, train_loss_vector, test_loss_vector = trainer.train(print_epochs=True)\n",
    "\n",
    "# Train MAE\n",
    "out = nn.forward(X_val_scaled)[-1][-1] \n",
    "out = y_scaler.inverse_transform(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d78f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nome team \"Fraleli\"\n",
    "# membri \"Alessandro Tesi, Elia Bocini, Francesco Fiaschi\"\n",
    "# data = \"21/01/2026\"\n",
    "\n",
    "df_submission = pd.DataFrame(out)\n",
    "df_submission.index = df_submission.index + 1  #adjust index to follow cup 25 csv\n",
    "df_submission.to_csv('FRALELI_ML-CUP25-TS.csv') ### RICORDATI DI AGGIUNGERE I COMMENTI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
