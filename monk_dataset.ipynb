{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940809f9",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12e541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==2.0.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: pandas==2.2.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.9.4)\n",
      "Requirement already satisfied: ucimlrepo==0.0.7 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.0.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in ./.venv/lib/python3.9/site-packages (from ucimlrepo==0.0.7->-r requirements.txt (line 4)) (2026.1.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (3.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (4.60.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (11.3.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (6.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 3)) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 2)) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f4bd5",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c2f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aut-reloader\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "from nn import NeuralNetwork, Trainer\n",
    "from utils.losses import Loss\n",
    "from utils.data_manage import DataLoader, StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945049e0",
   "metadata": {},
   "source": [
    "## Monk 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83be8f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/9hc35ffd1_b7r1ht502gn1r40000gn/T/ipykernel_40786/2338482450.py:4: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_train = pd.read_csv(PATH_TRAIN, delim_whitespace=True, header=None)\n",
      "/var/folders/5j/9hc35ffd1_b7r1ht502gn1r40000gn/T/ipykernel_40786/2338482450.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_test = pd.read_csv(PATH_TEST, delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "PATH_TRAIN = 'dataset/monks-1.train'\n",
    "PATH_TEST = 'dataset/monks-1.test'\n",
    "\n",
    "df_train = pd.read_csv(PATH_TRAIN, delim_whitespace=True, header=None)\n",
    "df_test = pd.read_csv(PATH_TEST, delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c356986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124, 6), (124, 1), (432, 6), (432, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert df in numpy arrays\n",
    "Train_set = np.array(df_train)\n",
    "Test_set = np.array(df_test)\n",
    "\n",
    "X_Train = Train_set[:, 1: -1].astype(int) # dalla prima alla penultima colonna, astype int usato se no np.eye non funziona\n",
    "y_Train = Train_set[:, 0].astype(int) # prima colonna\n",
    "y_Train = np.reshape(y_Train, (y_Train.shape[0], 1)) #(124,) -> (124,1)\n",
    "\n",
    "X_Test = Test_set[:, 1: -1].astype(int)\n",
    "y_Test = Test_set[:, 0].astype(int)\n",
    "y_Test = np.reshape(y_Test, (y_Test.shape[0], 1))\n",
    "\n",
    "X_Train.shape, y_Train.shape, X_Test.shape, y_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd0385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((124, 17), (432, 17))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encode\n",
    "X_train_onehot = []\n",
    "X_dataset = np.concatenate((X_Train, X_Test), axis=0)\n",
    "print(X_dataset.shape)\n",
    "for column in range(X_dataset.shape[1]):\n",
    "  X_dataset[:, column] = X_dataset[:, column] - 1 #needed to use np.eye\n",
    "  n_unique = len(np.unique(X_dataset[:, column]))\n",
    "  X_train_onehot.append(np.eye(n_unique)[X_dataset[:, column]])\n",
    "X_onehot = np.concatenate(X_train_onehot, axis=1)\n",
    "\n",
    "X_train = X_onehot[:X_Train.shape[0]]\n",
    "X_test = X_onehot[X_Train.shape[0]:]\n",
    "\n",
    "X_train.shape,  X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e97761a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSTICA ---\n",
      "1. Cartella di lavoro attuale (CWD): /Users/francescofiaschi/Library/CloudStorage/OneDrive-UniversityofPisa/Università/Magistrale/1°Anno/Machine Learning/Project/code/ProjectML_Course\n",
      "2. ⚠️ Trovata cartella (non file): utils/activations\n",
      "   Contenuto cartella: ['tanh.py', 'leaky_relu.py', '__init__.py', 'relu.py', 'sigmoid.py', 'identity.py', 'softmax.py']\n",
      "   -> C'è un __init__.py inside.\n",
      "--------------------\n",
      "3. Test importazione diretta:\n",
      "✅ Import riuscito! La funzione sigmoid è disponibile.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"--- DIAGNOSTICA ---\")\n",
    "print(f\"1. Cartella di lavoro attuale (CWD): {os.getcwd()}\")\n",
    "\n",
    "# Verifichiamo se esiste utils/activations.py o utils/activations/__init__.py\n",
    "path_act_file = os.path.join('utils', 'activations.py')\n",
    "path_act_folder = os.path.join('utils', 'activations')\n",
    "\n",
    "if os.path.isfile(path_act_file):\n",
    "    print(f\"2. ✅ Trovato file: {path_act_file}\")\n",
    "    with open(path_act_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        if \"def sigmoid\" in content:\n",
    "            print(\"   -> ✅ La funzione 'def sigmoid' è definita dentro.\")\n",
    "        else:\n",
    "            print(\"   -> ❌ ERRORE: Il file esiste ma NON contiene 'def sigmoid'.\")\n",
    "elif os.path.isdir(path_act_folder):\n",
    "    print(f\"2. ⚠️ Trovata cartella (non file): {path_act_folder}\")\n",
    "    print(\"   Contenuto cartella:\", os.listdir(path_act_folder))\n",
    "    # Check __init__.py\n",
    "    if \"__init__.py\" in os.listdir(path_act_folder):\n",
    "        print(\"   -> C'è un __init__.py inside.\")\n",
    "    else:\n",
    "        print(\"   -> ❌ ERRORE CRITICO: Cartella senza __init__.py. Python non può importare da qui.\")\n",
    "else:\n",
    "    print(f\"2. ❌ ERRORE GRAVE: Non trovo né {path_act_file} né la cartella {path_act_folder}\")\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(\"3. Test importazione diretta:\")\n",
    "try:\n",
    "    from utils.activations import sigmoid\n",
    "    print(\"✅ Import riuscito! La funzione sigmoid è disponibile.\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import fallito: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Altro errore: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34e8e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEURAL_NETWORK_ARCHITECTURE: [17, 8, 8, 1], NEURAL_NETWORK_ACTIVATION: ['tanh', 'tanh', 'identity'], LOSS_F: binary cross entropy sigmoid, ETA: 0.01, LAMBDA: 0, ALPHA: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescofiaschi/Library/CloudStorage/OneDrive-UniversityofPisa/Università/Magistrale/1°Anno/Machine Learning/Project/code/ProjectML_Course/utils/layers/layer.py:72: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return (self.weights @ input.T).T #output(batch_size, output_dim)\n",
      "/Users/francescofiaschi/Library/CloudStorage/OneDrive-UniversityofPisa/Università/Magistrale/1°Anno/Machine Learning/Project/code/ProjectML_Course/utils/layers/layer.py:72: RuntimeWarning: overflow encountered in matmul\n",
      "  return (self.weights @ input.T).T #output(batch_size, output_dim)\n",
      "/Users/francescofiaschi/Library/CloudStorage/OneDrive-UniversityofPisa/Università/Magistrale/1°Anno/Machine Learning/Project/code/ProjectML_Course/utils/layers/layer.py:72: RuntimeWarning: invalid value encountered in matmul\n",
      "  return (self.weights @ input.T).T #output(batch_size, output_dim)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     37\u001b[0m     nn\u001b[38;5;241m=\u001b[39mnn,\n\u001b[1;32m     38\u001b[0m     loss\u001b[38;5;241m=\u001b[39mLoss(LOSS_F),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     shuffle_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# return_best_nn=True returns the best nn\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m best_nn \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_best_nn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#best val accuracy\u001b[39;00m\n\u001b[1;32m     54\u001b[0m out \u001b[38;5;241m=\u001b[39m best_nn\u001b[38;5;241m.\u001b[39mforward(X_val)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofPisa/Università/Magistrale/1°Anno/Machine Learning/Project/code/ProjectML_Course/nn/trainer.py:50\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, min_clip, max_clip, return_best_nn, print_epochs, plot_epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[1;32m     49\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m---> 50\u001b[0m   tr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m   train_loss\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(tr_loss))\n\u001b[1;32m     53\u001b[0m   loss_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mcompute_loss_gradient(out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofPisa/Università/Magistrale/1°Anno/Machine Learning/Project/code/ProjectML_Course/utils/losses/loss.py:37\u001b[0m, in \u001b[0;36mLoss.compute_loss\u001b[0;34m(self, pred, actual)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, pred: np\u001b[38;5;241m.\u001b[39mndarray, actual: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m  Calcola la loss del batch\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    np.ndarray: loss del batch\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_dic\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_f\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactual\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofPisa/Università/Magistrale/1°Anno/Machine Learning/Project/code/ProjectML_Course/utils/losses/bce.py:33\u001b[0m, in \u001b[0;36msigmoid_binary_cross_entropy_loss\u001b[0;34m(pred, actual)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mSigmoid activates the input vector then B. Cross Entropy loss.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    actual (np.ndarray): actual class\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m actual\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted vector and Actual vector have two different lenghts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m binary_cross_entropy_loss(\u001b[43msigmoid\u001b[49m(pred), actual)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.activations.sigmoid import sigmoid\n",
    "\n",
    "monk_dataset = DataLoader(X_train, y_Train)\n",
    "K_FOLDS = monk_dataset.k_fold(k = 5)\n",
    "\n",
    "NEURAL_NETWORK_CONFIGURATIONS = [([17, 8, 8, 1],['tanh', 'tanh', 'identity'], 'binary cross entropy sigmoid'),\n",
    "                                 ([17, 8, 8, 1],['leaky relu', 'leaky relu', 'sigmoid'], 'mse'),\n",
    "                                 ]\n",
    "ETA_CONFIGURATIONS = [0.01, 0.001]\n",
    "LAMBDA_CONFIGURATIONS = [0, 1e-3]\n",
    "ALPHA_CONFIGURATIONS = [0, 0.5]\n",
    "EPOCHS = 300\n",
    "\n",
    "# (NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA)\n",
    "CONFIGURATIONS = []\n",
    "\n",
    "for NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F in NEURAL_NETWORK_CONFIGURATIONS:\n",
    "  for ETA in ETA_CONFIGURATIONS:\n",
    "    for LAMBDA in LAMBDA_CONFIGURATIONS:\n",
    "      for ALPHA in ALPHA_CONFIGURATIONS:\n",
    "        config = (NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA)\n",
    "        CONFIGURATIONS.append(config)\n",
    "\n",
    "CONFIG_DICTIONARY = {}\n",
    "\n",
    "for i in range(len(CONFIGURATIONS)):\n",
    "  CONFIG_DICTIONARY[i] = 0\n",
    "\n",
    "for i in range(len(CONFIGURATIONS)):\n",
    "  config = CONFIGURATIONS[i]\n",
    "  for X_train, y_train, X_val, y_val in K_FOLDS:\n",
    "    NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, LOSS_F, ETA, LAMBDA, ALPHA = config\n",
    "    print(f\"NEURAL_NETWORK_ARCHITECTURE: {NEURAL_NETWORK_ARCHITECTURE}, NEURAL_NETWORK_ACTIVATION: {NEURAL_NETWORK_ACTIVATION}, LOSS_F: {LOSS_F}, ETA: {ETA}, LAMBDA: {LAMBDA}, ALPHA: {ALPHA}\")\n",
    "    #train\n",
    "    nn = NeuralNetwork(NEURAL_NETWORK_ARCHITECTURE, NEURAL_NETWORK_ACTIVATION, std=0.2)\n",
    "    trainer = Trainer(\n",
    "        nn=nn,\n",
    "        loss=Loss(LOSS_F),\n",
    "        X_train=X_train,\n",
    "        y_train=y_train, #no scaling y because of onehot\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        epochs=EPOCHS,\n",
    "        early_stopping=100, # no improvements in 50 epochs_> stop\n",
    "        eta=ETA,               # Learning rate iniziale\n",
    "        lam=LAMBDA,                # L2\n",
    "        alpha=ALPHA,               # Momentum\n",
    "        batch_size=16,\n",
    "        shuffle_batches=True\n",
    "    )\n",
    "    # return_best_nn=True returns the best nn\n",
    "    best_nn = trainer.train(return_best_nn=True, print_epochs=False, plot_epochs=True)\n",
    "    #best val accuracy\n",
    "    out = best_nn.forward(X_val)[-1][-1]\n",
    "    if LOSS_F == 'binary cross entropy sigmoid':\n",
    "      out = sigmoid(out)\n",
    "\n",
    "    predictions = np.round(out)\n",
    "    print(f\"Accuracy: {np.mean(predictions == y_val) * 100}%\")\n",
    "    print(CONFIG_DICTIONARY[i] )\n",
    "    CONFIG_DICTIONARY[i] += np.mean(predictions == y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(CONFIGURATIONS)):\n",
    "  print(f\"configuration: {CONFIGURATIONS[i]}, mean accuracy: {CONFIG_DICTIONARY[i] * 20}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
